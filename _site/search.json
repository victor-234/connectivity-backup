[
  {
    "objectID": "output-analyses.html",
    "href": "output-analyses.html",
    "title": "Technical stuff",
    "section": "",
    "text": "# ---\n# title: \"Connectivity Analyses\"\n# author: [\"Maximilian A. Müller\", \"Gaizka Ormazabal\", \"Thorsten Sellhorn\", \"Victor Wagner\"]\n# institute: [\"University of Cologne\", \"IESE\", \"LMU Munich School of Management\", \"LMU Munich School of Management\"]\n# number-sections: true\n\n# toc: true\n# format:\n#     pdf:\n#         geometry:\n#             - paper = a4paper\n#         fig-pos: 'h'\n#     html:\n#         toc: true\n# jupyter: python3\n# fig-width: 0.7\n# ---\n# echo: false\n# output: asis\n\n# # pdf:\n# #         geometry:\n# #             - paper=a4paper\n\n# beamer:\n#         aspectratio: 169\n#         header-includes: |\n#             \\setbeamertemplate{navigation symbols}{}\n#             \\setbeamertemplate{footline}[page number]"
  },
  {
    "objectID": "output-analyses.html#getting-started",
    "href": "output-analyses.html#getting-started",
    "title": "Technical stuff",
    "section": "Getting started",
    "text": "Getting started\n\nimports\nSRN data\n(check which firms and documents are needed and which are already here); Currently, we base our analyses on the documents that have already been manually coded (bottom-up); in the future, however, we do it top-down based on Euro Stoxx 600\nread in document data\n\nread in manually coded documents\ncheck which have to be downloaded (b/c they are not here) and update\ncheck which have to be extracted (b/c it hasn’t been done) and update\nprepare for later analysis and merge with other data\n\n\n\nimport sys, os, json, re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# path to Dropbox directory with 'pdf', 'pdf-decrypt', 'docs_clean.json' and 'raw_texts.json'\nBASE_PATH = \"/Users/victor.wagner/Dropbox/_Connectivity-Data\"\n\n\nAPI_PATH  = \"https://api.sustainabilityreportingnavigator.com/api/\"\n\ncompanies = pd.read_json(f'{API_PATH}companies')\ndocuments = pd.read_json(f'{API_PATH}documents')\nindices   = pd.read_json(f'{API_PATH}indices')\n\n\n# es600 = companies[['1cc738c1-e6b1-4f2b-8bec-2d963118de59' in ind for ind in companies.indices]]\n\n# es600[['id', 'name', 'isin', 'country', 'sector']].merge(\n#     documents[['id', 'year', 'type', 'company_id']], \n#     left_on='id', right_on='company_id',\n#     suffixes=('_company', '_document')\n# ).query()\n\n\nRead in manually collected start/end pages\n\nfrom mergeManualCollections import mergeManualCollections\nmanual_collections = mergeManualCollections([\n    '../data/connectivity-manual_collection - lmu_cologne - 20230926.csv',\n    '../data/connectivity-manual_collection-iese - main - 20230926.csv',\n    '../data/connectivity-manual_collection-iese - second_round - 20230926.csv',\n])\n\nprint('We have', len(manual_collections), 'manually coded documents, of which', \n      len(set(manual_collections.index).difference(set([x[:36] for x in os.listdir(BASE_PATH + \"/pdf/\")]))),\n      'are not downloaded.')\n\n\nUse the following with caution, this will download, extract and clean all documents if they are not stored locally.\n\n\n\nUpdate pdfs if not all are downloaded yet\n\nfrom downloadPdfs import downloadPdfs\ndoc_status = downloadPdfs(\n    BASE_PATH,\n    zip(manual_collections['href_doc'],\n        manual_collections.index)\n)\nprint('There were', sum([x == 'downloaded' for x in doc_status]), 'new downloads,', \n      sum([x == 'no download' for x in doc_status]), 'document could not be downloaded.')\n\nThere were 51 new downloads, 1 document could not be downloaded.\n\n\n\n\nExtract text from pdfs if not done so yet\n\nraw_texts_old = pd.read_json(f\"{BASE_PATH}/pdf/../raw_texts.json\")\nprint('We already have raw text for', len(raw_texts_old), 'documents, decrypting and reading',\n     len(manual_collections)-len(raw_texts_old), 'documents now.')\n\nfrom readTextFromPdf import readTextFromPdf\nraw_texts_new, doc_status_new = [], []\nraw_texts_new, doc_status_new = map(\n    list, \n    zip(*[readTextFromPdf(BASE_PATH, doc_id) for doc_id in manual_collections.index])\n)\nraw_texts_newDict = {doc_id: (r,s) for doc_id,r,s in zip(manual_collections.index, raw_texts_new, doc_status_new)}\n\nWe already have raw text for 961 documents, decrypting and reading 0 documents now.\n\n\n\nfrom updateRawTexts import updateRawTexts\nraw_texts_final = updateRawTexts(raw_texts_old, raw_texts_newDict)\nraw_texts_final.to_json(f\"{BASE_PATH}/pdf/../raw_texts.json\")\n\nprint('There are', sum([x == 'problem_decrypting' for x in raw_texts_final.status]), 'documents with decryption problems', \n     'and', sum([x == 'problem_opening' for x in raw_texts_final.status]), 'documents that could not be read.')\nprint('This leaves us with a readable sample of', sum([x == 'fine' for x in raw_texts_final.status]), 'documents.')\n\nThere are 184 documents with decryption problems and 0 documents that could not be read.\nThis leaves us with a readable sample of 777 documents.\n\n\n\n\nClean text if not done so yet\n\ndocs_cleaned_old = pd.read_json(f\"{BASE_PATH}/docs_clean.json\")\nprint('There is already clean data for', len(docs_cleaned_old), 'documents, cleaning',\n     len(raw_texts_final.query('status == \"fine\"'))-len(docs_cleaned_old), 'documents.')\n\nfrom updateAndCleanText import updateAndCleanText\ndocs_cleaned_new = updateAndCleanText(docs_cleaned_old, raw_texts_final)\ndocs_cleaned_new.to_json(f\"{BASE_PATH}/pdf/../docs_clean.json\")\n\nThere is already clean data for 961 documents, cleaning -184 documents.\n\n\n\ndel raw_texts_final, raw_texts_new, raw_texts_newDict, raw_texts_old\ndel docs_cleaned_old\n\n\n\nMerge other data to documents\n\n#docs_cleaned_new.drop(columns=['clean_text_full'], inplace=True)\n\ndocs = docs_cleaned_new.merge(\n    documents[['id', 'company_id', 'year']], \n    left_index=True, right_on='id'\n).merge(\n    companies[['id', 'name', 'isin', 'country', 'sector']], \n    left_on='company_id', right_on='id',\n    suffixes=('', '_company')\n).set_index(\n    'id', drop=True\n).drop(\n    'id_company', axis=1\n)\n\n\n\nMerge manually coded section data\n\ndocs = docs.merge(\n    manual_collections[['mda_begin', 'mda_end', 'fs_begin', 'fs_end', 'audit_begin', 'audit_end']], \n    left_index=True, right_index=True\n)\n\n\ndocs = docs.reset_index(\n).merge(\n    docs.groupby(\n        ['company_id']).size().reset_index(\n    ).rename(\n        columns={0: 'company_years_avlbl'}), on='company_id'\n).set_index('index')\n\n# Drop obvious outlier\ndocs.drop(docs[(docs['isin'] == 'ES0130670112') & (docs['year'] == 2020)].index, inplace=True)"
  },
  {
    "objectID": "output-analyses.html#sec-doc-sample",
    "href": "output-analyses.html#sec-doc-sample",
    "title": "Technical stuff",
    "section": "Document sample selection",
    "text": "Document sample selection\nWe distributed the EURO Stoxx 600 firms to student assistants to extract information on the structure of annual reports. So far, this resulted in the following documents sample: 1. no metadata in the SRN database yet, 1. after removing not readable pdfs, 2. pdfs where no text was scanned, and 3. pdfs with too little words (documents where the average clean text characters are less than 1,500 in a document).\n\ndocs0 = docs.query('status == \"fine\"')\ndocs1 = docs0.query('clean_text_len &gt; 0')\ndocs2 = docs1.query('avgCleanTextPerPage &gt; 1500')\n\nTpdfSample = pd.DataFrame(columns=['', 'less', 'resulting'])\n\nTpdfSample.loc[len(TpdfSample),:] = ['downloaded pdfs',      '',                             len(docs_cleaned_new)]\nTpdfSample.loc[len(TpdfSample),:] = ['no metadata',       '-'+str(len(docs_cleaned_new) -len(docs)), len(docs)]\nTpdfSample.loc[len(TpdfSample),:] = ['not readable',      '-'+str(len(docs) -len(docs0)), len(docs0)]\nTpdfSample.loc[len(TpdfSample),:] = ['zero text scanned', '-'+str(len(docs0)-len(docs1)), len(docs1)]\nTpdfSample.loc[len(TpdfSample),:] = ['too little words*', '-'+str(len(docs1)-len(docs2)), len(docs2)]\n\n#print(TpdfSample.to_markdown(index=False))\n#print('\\nThis leaves us with', len(docs2['company_id'].unique()), 'unique firms.')\n\n\nTpdfSample.reset_index(drop=True)\n\n\n\n\n\n\nTable 1: Sample selection documents\n\n\n\n\nless\nresulting\n\n\n\n\n0\ndownloaded pdfs\n\n961\n\n\n1\nno metadata\n-53\n908\n\n\n2\nnot readable\n-168\n740\n\n\n3\nzero text scanned\n-28\n712\n\n\n4\ntoo little words*\n-102\n610"
  },
  {
    "objectID": "output-analyses.html#keyword-search",
    "href": "output-analyses.html#keyword-search",
    "title": "Technical stuff",
    "section": "Keyword search",
    "text": "Keyword search\n\nfrom findSections import findSections\npd.options.mode.chained_assignment = None  # default='warn'\n\ndocsa = findSections(docs2)\n\n\nsearch_patterns = ['ghg', 'co2', 'carbon', 'climat', 'emission', 'regulatory risk', 'physical risk', 'transition risk']\nprint(search_patterns)\n\n['ghg', 'co2', 'carbon', 'climat', 'emission', 'regulatory risk', 'physical risk', 'transition risk']\n\n\n\nlim = 200\n\ndef searchText(doc, search_patterns):\n    matches = []\n    for section in ['mda_text', 'fs_text', 'audit_text', 'other_text']:\n        for pat in search_patterns:\n            for page, text in doc[section+'_dict'].items():\n                for hit in re.finditer(pat, text):\n                    matches.append({\n                        'pattern': pat,\n                        'section': section[:-5],\n                        'snippet': text[hit.start()-lim : hit.start()+len(pat)+lim],\n                        'page'   : page\n                    })\n                \n    return matches\n\ndocsa['hits'] = [pd.DataFrame(searchText(doc, search_patterns)) for _, doc in docsa.iterrows()]\n\n\n# for sec in sections:\n#     docsamax[sec+'_hit'] = [len(list(filter(lambda hit: hit['section'] == sec, searchText(doc, search_patterns)))) for _, doc in docsa.iterrows()]\n    \n# for pat in search_patterns:\n#     docsamax[pat+'_hit'] = [len(list(filter(lambda hit: hit['pattern'] == pat, searchText(doc, search_patterns)))) for _, doc in docsa.iterrows()]\n#     docsamax[pat+'_fs_hit'] = [len(list(filter(lambda hit: (hit['pattern'] == pat) and (hit['section'] == 'fs'), searchText(doc, search_patterns)))) for _, doc in docsa.iterrows()]\n    \n\n\n# snippets = pd.DataFrame(columns=['document_id', 'isin', 'year', 'pattern', 'section', 'snippet'])\n\n# for idx, doc in docsa.iterrows():\n#     for _, hit in doc.hits.iterrows():\n#         snippets.loc[len(snippets)] = [idx, doc['isin'], doc['year'], hit['pattern'], hit['section'], hit['snippet']]\n    \n# snippets.info()\n# snippets.to_csv('snippets.csv', index=False)\n\n\npd.read_csv('../data/worldscope_msci_carbon.csv')"
  },
  {
    "objectID": "output-analyses.html#sample-and-method",
    "href": "output-analyses.html#sample-and-method",
    "title": "Technical stuff",
    "section": "Sample and method",
    "text": "Sample and method\n\nWe downloaded 961 annual reports from Euro Stoxx 600 firms in pdf format\nAfter cleaning (see more Section 0.2), we were able to extract text from 611 documents\nWe then manually classified different sections of these reports\n\nMD&A (a.k.a. management report, Lagebericht, etc.)\nFinancial Statements and Notes\nAuditor’s Report\n\nAnd searched for climate-related keywords"
  },
  {
    "objectID": "output-analyses.html#keywords-over-time",
    "href": "output-analyses.html#keywords-over-time",
    "title": "Technical stuff",
    "section": "Keywords over time",
    "text": "Keywords over time\n\ndata = docsa.copy()\n\nfor sec in sections:\n    data[sec+'_hits'] = [len(list(filter(lambda hit: hit['section'] == sec, searchText(doc, search_patterns)))) for _, doc in docsa.iterrows()]\n\ndata['total_hits'] = [len(searchText(doc, search_patterns)) for _, doc in docsa.iterrows()]\n\n\navg_hits_perSec_perYear = [[np.mean(data.query('year == @year')[sec+'_hits']) for year in years] for sec in sections]\nmed_hits_perSec_perYear = [[np.median(data.query('year == @year')[sec+'_hits']) for year in years] for sec in sections]\navg_hitsPerPage_perSec_perYear = [[np.mean(data.query('year == @year')[sec+'_hits']/data.query('year == @year')['n_pages']) for year in years] for sec in sections]\n\n\nfs_hits_avg_yrs =    [np.mean(data.query('year == @year')['fs_hits'])    for year in years]\naudit_hits_avg_yrs = [np.mean(data.query('year == @year')['audit_hits']) for year in years]\nmda_hits_avg_yrs =   [np.mean(data.query('year == @year')['mda_hits'])   for year in years]\nother_hits_avg_yrs = [np.mean(data.query('year == @year')['other_hits']) for year in years]\n\nfs_hits_med_yrs =    [np.median(data.query('year == @year')['fs_hits'])    for year in years]\naudit_hits_med_yrs = [np.median(data.query('year == @year')['audit_hits']) for year in years]\nmda_hits_med_yrs =   [np.median(data.query('year == @year')['mda_hits'])   for year in years]\nother_hits_med_yrs = [np.median(data.query('year == @year')['other_hits']) for year in years]\n\n\nfs_hits_byY = [data.query('year == @year')['fs_hits'].values for year in years]\nfs_hits_ci  = [stats.t.interval(0.95, len(hits_byY)-1, loc=hits_byY.mean(), scale=hits_byY.std()) for hits_byY in fs_hits_byY]\nfs_hits_iqr = [stats.iqr(x) for x in fs_hits_byY]\nfs_hits_q1  = [np.quantile(data.query('year == @year')['fs_hits'], 0.25) for year in years]\nfs_hits_q3  = [np.quantile(data.query('year == @year')['fs_hits'], 0.75) for year in years]\n\naudit_hits_byY = [data.query('year == @year')['audit_hits'].values for year in years]\naudit_hits_ci  = [stats.t.interval(0.95, len(hits_byY)-1, loc=hits_byY.mean(), scale=hits_byY.std()) for hits_byY in audit_hits_byY]\naudit_hits_iqr = [stats.iqr(x) for x in audit_hits_byY]\naudit_hits_q1  = [np.quantile(data.query('year == @year')['audit_hits'], 0.25) for year in years]\naudit_hits_q3  = [np.quantile(data.query('year == @year')['audit_hits'], 0.75) for year in years]\n\n\nfig, ax1 = plt.subplots()\n\nax1.plot(years, med_hits_perSec_perYear[0], 'o-', color='#ff1f5b', label='fs+notes')\nax1.plot(years, med_hits_perSec_perYear[1], 'o-', color='#009ade', label='audit')\n\n# for year, n in zip(years, range(len(years))):\n#     ax1.plot(\n#         (year, year), \n#         (med_hits_perSec_perYear[0][n]+fs_hits_q3[n], \n#          med_hits_perSec_perYear[0][n]-fs_hits_q1[n]), \n#         '_-', color='#ff1f5b', alpha=0.5\n#     )\n#     ax1.plot(\n#         (year, year), \n#         (med_hits_perSec_perYear[1][n]+audit_hits_q3[n], \n#          med_hits_perSec_perYear[1][n]-audit_hits_q1[n]), \n#         '_-', color='#009ade', alpha=0.5\n#     )\n\nax1.set_ylabel('median hits per section')\n\ndocsPerYear = [len(data.query('year == @year')) for year in years]\nax2 = ax1.twinx()\nax2.bar(x=years, height=docsPerYear, color='gray', alpha=0.2)\nax2.set_ylim([0,600])\nax2.tick_params(axis='y')\nax2.set_ylabel('#docs')\n\nplt.table([[round(y, 2) for y in x] for x in med_hits_perSec_perYear+[docsPerYear]],\n         rowLabels=['fs+notes', 'audit', 'mda', 'other', '#docs'],\n         rowColours=['#ff1f5b', '#009ade', '#ffffff', '#ffffff', 'lightgray'],\n         colLabels=years, loc='bottom', cellLoc='right', edges='horizontal', bbox=[0, -0.4, 1, 0.4])\n\nax1.set_xticks([])\nax2.set_xticks([])\n#ax1.set_title(f'n={len(data)} reports with {sum(data[\"total_hits\"])} total keyword hits with IQR')\nax1.legend()#ncol=4, bbox_to_anchor=(0.75,-0.1))\n\n#fig.savefig('figures/fig202309271823.png', dpi=400, bbox_inches='tight')\nplt.show()\n\n\n\n\n\nfrom scipy.interpolate import make_interp_spline\nsplined_fs    = make_interp_spline(years, avg_hits_perSec_perYear[0])\nsplined_audit = make_interp_spline(years, avg_hits_perSec_perYear[1])\n\nyears_spl = np.linspace(min(years), max(years), 500)\ny_fs      = splined_fs(years_spl)\ny_audit   = splined_audit(years_spl)\n\n\nfig, ax1 = plt.subplots()\n\nax1.plot(years_spl, y_fs, '-', color='#ff1f5b', label='fs+notes')\nax1.plot(years_spl, y_audit, '-', color='#009ade', label='audit')\n\n# for year, n in zip(years, range(len(years))):\n#     ax1.plot(\n#         (year, year), \n#         (avg_hits_perSec_perYear[0][n]+fs_hits_ci[n][0], \n#          avg_hits_perSec_perYear[0][n]+fs_hits_ci[n][1]), \n#         '_-', color='#ff1f5b', alpha=0.5)\n#     ax1.plot(\n#         (year, year), \n#         (avg_hits_perSec_perYear[1][n]+audit_hits_ci[n], \n#          avg_hits_perSec_perYear[1][n]+audit_hits_ci[n]), \n#         '_-', color='#009ade', alpha=0.5)\n\nax1.set_ylabel('average hits per section')\n\ndocsPerYear = [len(data.query('year == @year')) for year in years]\nax2 = ax1.twinx()\nax2.bar(x=years, height=docsPerYear, color='gray', alpha=0.2)\nax2.set_ylim([0,600])\nax2.tick_params(axis='y')\nax2.set_ylabel('#docs')\n\nplt.table([[round(y, 2) for y in x] for x in avg_hits_perSec_perYear+[docsPerYear]],\n         rowLabels=['fs+notes', 'audit', 'mda', 'other', '#docs'],\n         rowColours=['#ff1f5b', '#009ade', '#ffffff', '#ffffff', 'lightgray'],\n         colLabels=years, loc='bottom', cellLoc='right', edges='horizontal', bbox=[0, -0.4, 1, 0.4])\n\nax1.set_xticks([])\nax2.set_xticks([])\n#ax1.set_title(f'n={len(data)} reports with {sum(data[\"total_hits\"])} total keyword hits with IQR')\nax1.legend()#ncol=4, bbox_to_anchor=(0.75,-0.1))\n\n#fig.savefig('figures/fig202309280927.png', dpi=400, bbox_inches='tight')\nplt.show()\n\n\n\n\nFigure 1: Development of average hits\n\n\n\n\n\nfig, ax1 = plt.subplots()\n\nax1.plot(years, avg_hits_perSec_perYear[0],    'o-', color='#ff1f5b', label='fs+notes')\nax1.plot(years, avg_hits_perSec_perYear[1], 'o-', color='#009ade', label='audit')\nax1.plot(years, avg_hits_perSec_perYear[2],   'o-', color='#af58ba', label='mda')\nax1.plot(years, avg_hits_perSec_perYear[3], 'o-', color='#ffc61e', label='other')\n\nax1.set_ylabel('average hits per section')\n\ndocsPerYear = [len(data.query('year == @year')) for year in years]\nax2 = ax1.twinx()\nax2.bar(x=years, height=docsPerYear, color='gray', alpha=0.2)\nax2.set_ylim([0,600])\nax2.tick_params(axis='y')\nax2.set_ylabel('#docs')\n\nplt.table([[round(y, 2) for y in x] for x in avg_hits_perSec_perYear+[docsPerYear]],\n         rowLabels=['fs+notes', 'audit', 'mda', 'other', '#docs'],\n         rowColours=['#ff1f5b', '#009ade', '#af58ba', '#ffc61e', 'lightgray'],\n         colLabels=years, loc='bottom', cellLoc='right', bbox=[0, -0.4, 1, 0.4])\n\nax1.set_xticks([])\nax2.set_xticks([])\n#ax1.set_title(f'n={len(data)} reports with {sum(data[\"total_hits\"])} total keyword hits')\nax1.legend()#ncol=4, bbox_to_anchor=(0.75,-0.1))\n\n#fig.savefig('figures/fig202309272029.png', dpi=400, bbox_inches='tight')\nplt.show()\n\n\n\n\nFigure 2: Development of average hits\n\n\n\n\n\nfig, ax1 = plt.subplots()\n\nax1.plot(years, avg_hitsPerPage_perSec_perYear[0], 'o-', color='#ff1f5b', label='fs+notes')\nax1.plot(years, avg_hitsPerPage_perSec_perYear[1], 'o-', color='#009ade', label='audit')\nax1.plot(years, avg_hitsPerPage_perSec_perYear[2], 'o-', color='#af58ba', label='mda')\nax1.plot(years, avg_hitsPerPage_perSec_perYear[3], 'o-', color='#ffc61e', label='other')\n\nax1.set_ylabel('average hits per page by section')\n\ndocsPerYear = [len(data.query('year == @year')) for year in years]\nax2 = ax1.twinx()\nax2.bar(x=years, height=docsPerYear, color='gray', alpha=0.2)\nax2.set_ylim([0,600])\nax2.tick_params(axis='y')\nax2.set_ylabel('#docs')\n\nplt.table([[round(y, 2) for y in x] for x in avg_hitsPerPage_perSec_perYear+[docsPerYear]],\n         rowLabels=['fs+notes', 'audit', 'mda', 'other', '#docs'],\n         rowColours=['#ff1f5b', '#009ade', '#af58ba', '#ffc61e', 'lightgray'],\n         colLabels=years, loc='bottom', cellLoc='right', bbox=[0, -0.4, 1, 0.4])\n\nax1.set_xticks([])\nax2.set_xticks([])\n#ax1.set_title(f'n={len(data)} reports with {sum(data[\"total_hits\"])} total keyword hits')\nax1.legend()#ncol=4, bbox_to_anchor=(0.75,-0.1))\n\n#fig.savefig('figures/fig202309272029.png', dpi=400, bbox_inches='tight')\nplt.show()\n\n\n\n\nFigure 3: Development of hits per page"
  },
  {
    "objectID": "output-analyses.html#keyword-distribution-in-the-reports",
    "href": "output-analyses.html#keyword-distribution-in-the-reports",
    "title": "Technical stuff",
    "section": "Keyword distribution in the reports",
    "text": "Keyword distribution in the reports\n\n# fig, ax = plt.subplots(1,4,figsize=(12,3), sharey=True)\n\n# #ax[0].hist(data.total_hits, color='lightgray')\n# ax[0].hist(data.fs_hits,    color='#ff1f5b')\n# ax[1].hist(data.audit_hits, color='#009ade')\n# ax[2].hist(data.mda_hits,   color='#af58ba')\n# ax[3].hist(data.other_hits, color='#ffc61e')\n\n# for n, title in zip(range(4), ['fs+notes', 'audit', 'mda', 'other']):\n#     ax[n].set_title(title)\n\n# plt.suptitle(f'Distribution of hits per document by section', y=1.1)\n# plt.savefig('figures/fig202309271836.png', dpi=400, bbox_inches='tight')"
  },
  {
    "objectID": "output-analyses.html#keyword-ranking",
    "href": "output-analyses.html#keyword-ranking",
    "title": "Technical stuff",
    "section": "Keyword ranking",
    "text": "Keyword ranking\n\ndata = docsa.copy()\n\nfor pat in search_patterns:\n    data[pat] = [len(list(filter(lambda hit: hit['pattern'] == pat, searchText(doc, search_patterns)))) for _, doc in docsa.iterrows()]\n    \ndata['total_hits'] = [len(searchText(doc, search_patterns)) for _, doc in docsa.iterrows()]\n\n\nfig, ax = plt.subplots()\n\nax.bar(search_patterns, [np.sum(data[pat].values) for pat in search_patterns], color='#a0b1ba', zorder=2)\n#ax.set_title(f'Distribution of hits per keyword (total hits={sum(data.total_hits)})')\nax.text(6.45,30500,f'n={sum(data.total_hits)}')\n\nplt.grid(axis='y', color='lightgray', zorder=0)\n\nplt.xticks(rotation=90)\nplt.savefig('figures/fig202309272100.png', dpi=400, bbox_inches='tight')\n\n\n\n\nFigure 4: keyword ranking"
  },
  {
    "objectID": "output-analyses.html#keyword-ranking-by-section",
    "href": "output-analyses.html#keyword-ranking-by-section",
    "title": "Technical stuff",
    "section": "Keyword ranking by section",
    "text": "Keyword ranking by section\nThe difference b/w the total number of search hits is because from here, I included ghg and co2\n\nsections = ['fs', 'audit', 'mda', 'other']\n\ntotals = {k:v for k,v in zip(\n    sections,\n    [[sum(len(list(filter(lambda hit: (hit['section'] == sec) & (hit['pattern'] == pat), searchText(doc, search_patterns))))\n          for _, doc in docsa.iterrows()) \n      for pat in search_patterns] \n     for sec in sections]\n)}\n\n\nimport matplotlib as mpl\nmpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=['#ff1f5b', '#009ade', '#af58ba', '#ffc61e'])\n\nfig, ax = plt.subplots()\n\nwidth = 0.2\nlabel_locs = np.arange(len(search_patterns))\nmultiplier = 0\n\nfor sec in list(totals.keys())[:2]:\n    offset = width * multiplier\n    ax.bar(label_locs+offset, totals[sec], width=width, label=sec, edgecolor='w')\n    #ax.bar_label(rects, padding=3)\n    multiplier += 1\n    \nax.set_xticks(label_locs+width)\nax.set_xticklabels(search_patterns)\nplt.xticks(rotation=90)\nax.legend()\n\nplt.savefig('figures/fig202309272158.png', dpi=400, bbox_inches='tight')\n\n\n\n\nFigure 5: Keyword ranking by section\n\n\n\n\n\nfig, ax = plt.subplots()\n\nwidth = 0.2\nlabel_locs = np.arange(len(search_patterns))\nmultiplier = 0\n\nfor sec, byKeyword in totals.items():\n    offset = width * multiplier\n    ax.bar(label_locs+offset, byKeyword, width=width, label=sec, edgecolor='w')\n    #ax.bar_label(rects, padding=3)\n    multiplier += 1\n    \nax.set_xticks(label_locs+width)\nax.set_xticklabels(search_patterns)\nplt.xticks(rotation=90)\nax.legend(loc='upper right')\n\nplt.savefig('figures/fig202309272147.png', dpi=400, bbox_inches='tight')\n\n\n\n\nFigure 6: Keyword ranking by section"
  },
  {
    "objectID": "output-analyses.html#heatmap-of-hits-in-the-document",
    "href": "output-analyses.html#heatmap-of-hits-in-the-document",
    "title": "Technical stuff",
    "section": "Heatmap of hits in the document",
    "text": "Heatmap of hits in the document\n\ndata1 = pd.DataFrame(columns=[\n    'document_id', 'company_id', 'year', 'fs_begin', 'fs_end', 'audit_begin', 'audit_end', \n    'pattern', 'section', 'page'\n])\n\nfor doc_id, doc in docsa.iterrows():\n    for _, hit in doc.hits.iterrows():\n        if (hit.section == 'fs') or (hit.section == 'audit'):\n            data1.loc[len(data1)] = [\n                doc_id, doc.company_id, doc.year, doc.fs_begin, doc.fs_end, doc.audit_begin, doc.audit_end,\n                hit.pattern, hit.section, hit.page\n            ]\n\n\ndata1_onlyFS    = data1.query('section == \"fs\"').drop(['audit_begin', 'audit_end', 'section'], axis=1)\ndata1_onlyaudit = data1.query('section == \"audit\"').drop(['fs_begin', 'fs_end', 'section'], axis=1)\n\n\ndata1_onlyFS_noMult = data1_onlyFS[[',' not in str(x) for x in data1_onlyFS['fs_begin']]]\ndata1_onlyaudit_noMult = data1_onlyaudit[[',' not in str(x) for x in data1_onlyaudit['audit_begin']]]\nprint('By restricting to no multi-start sections, we lose', \n      len(data1_onlyFS)-len(data1_onlyFS_noMult), 'snippets.\\n',\n      'Left:', len(data1_onlyFS_noMult))\n\nBy restricting to no multi-start sections, we lose 34 snippets.\n Left: 4178\n\n\n\ndata1_onlyFS_noMult['location'] = [int(snip['page']) / int(snip['fs_end']) for _,snip in data1_onlyFS_noMult.iterrows()]\ndata1_onlyaudit_noMult['location'] = [int(snip['page']) / int(snip['audit_end']) for _,snip in data1_onlyaudit_noMult.iterrows()]\n\n\nfig, ax = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(6,6))\n\ncustomprops = dict(linestyle='--', linewidth=1, color='gray')\n\n# ax.boxplot(data1_onlyFS_noMult['location'], showfliers=0, showmeans=1, meanline=1, showcaps=0, \n#            boxprops=customprops, whiskerprops=customprops, capprops=customprops, \n#            medianprops=dict(color='#ff1f5b'), meanprops=dict(linestyle=':', linewidth=1, color='gray'))\n\nfor n, year in enumerate([2018, 2020, 2022]):\n    sns.stripplot(data=data1_onlyFS_noMult.query('year == @year'), \n                  x='location', y='pattern', \n                  color=colors[n], alpha=0.4, size=2.5, ax=ax[n], order=search_patterns)\n    ax[n].grid(color='lightgray', axis='y', linestyle='dashed')\n    ax[n].set_ylabel('')\n    ax[n].set_title(year)\n    \n\nax[0].set_xlim([0,1])\nax[2].set_xlabel('')\n\nplt.show()\n\n\n\n\nFigure 7: Relative location"
  },
  {
    "objectID": "output-analyses.html#cross-sectional-splits",
    "href": "output-analyses.html#cross-sectional-splits",
    "title": "Technical stuff",
    "section": "Cross-sectional splits",
    "text": "Cross-sectional splits\n\nsnippets[snippets.document_id == 'f6d019db-5b4d-4b4b-b989-427262d0873b'].query('section == \"fs\"')\n\n\n\n\n\n\n\n\ndocument_id\nisin\nyear\npattern\nsection\nsnippet\n\n\n\n\n87779\nf6d019db-5b4d-4b4b-b989-427262d0873b\nDE000DTR0CK8\n2021\ncarbon\nfs\njoint venture in january 2022, daimler truck ...\n\n\n87780\nf6d019db-5b4d-4b4b-b989-427262d0873b\nDE000DTR0CK8\n2021\nemission\nfs\nt can only remain successful over the longterm...\n\n\n87781\nf6d019db-5b4d-4b4b-b989-427262d0873b\nDE000DTR0CK8\n2021\nemission\nfs\nainable corporate governance. this includes th...\n\n\n87782\nf6d019db-5b4d-4b4b-b989-427262d0873b\nDE000DTR0CK8\n2021\nemission\nfs\nechnology path in the future and explore the p...\n\n\n87783\nf6d019db-5b4d-4b4b-b989-427262d0873b\nDE000DTR0CK8\n2021\nemission\nfs\norecasted planning periods as well as within c...\n\n\n87784\nf6d019db-5b4d-4b4b-b989-427262d0873b\nDE000DTR0CK8\n2021\nemission\nfs\nher european truck manufacturers for their par..."
  },
  {
    "objectID": "output-analyses.html#interactive",
    "href": "output-analyses.html#interactive",
    "title": "Technical stuff",
    "section": "Interactive",
    "text": "Interactive\n\n# #| fig-cap: Average keyword hits per section over time\n# fig, ax1 = plt.subplots()\n\n# ax1.plot(years, avg_hits_perSec_perYear[0], 'o-', color='#ff1f5b', label='fs+notes')\n# ax1.plot(years, avg_hits_perSec_perYear[1], 'o-', color='#009ade', label='audit')\n\n# ax1.set_ylabel('average hits per section')\n# ax1.set_xticks(years)\n# ax1.legend()\n\n# plt.show()"
  },
  {
    "objectID": "output-analyses.html#document-descriptives",
    "href": "output-analyses.html#document-descriptives",
    "title": "Technical stuff",
    "section": "Document descriptives",
    "text": "Document descriptives\n\nfig, ax = plt.subplots(1,3, figsize=(12,3))\n\nax[0].hist(docsa['n_pages'], edgecolor=\"w\", bins=25, color='#a0b1ba')\nax[0].set_title(f'Panel A. Total page numbers')\n\nax[1].hist(docsa['clean_text_len'], edgecolor=\"w\", bins=25, color='#a0b1ba')\nax[1].set_title(f'Panel B. Clean text lengths')\n\nax[2].hist(docsa['avgCleanTextPerPage'], edgecolor=\"w\", bins=25, color='#a0b1ba')\nax[2].set_title(f'Panel C. Clean text lengths per page')\nax[2].text(x=8400, y=-20, s=f'Cropped at 1,500', ha='right')\n\nfig.suptitle(f\"n={len(docsa)} documents\", y=1.1)\nplt.show()\n#fig.savefig('figures/fig202309272049.png', dpi=400, bbox_inches='tight')\n\n\n\n\nFigure 8: Document descriptives\n\n\n\n\n\nimport nltk\n\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\n\nx = sent_tokenize(docsa.iloc[0]['fs_text'])\nx"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Connectivity Analyses",
    "section": "",
    "text": "Nothing here, yet."
  },
  {
    "objectID": "index.html#method",
    "href": "index.html#method",
    "title": "Connectivity Analyses",
    "section": "Method",
    "text": "Method\nWe downloaded 961 annual reports of EURO Stoxx 600 firms from 2018 to 2022.\nWe then manually classified different sections of the reports:\n\nMD&A (a.k.a. management report, Lagebericht, etc.)\nFinancial Statements and Notes\nAuditor’s Report\n\nFinally, we searched for different climate-related keywords:\n\n\n\n['ghg', 'co2', 'carbon', 'climat', 'emission', 'regulatory risk', 'physical risk', 'transition risk']"
  },
  {
    "objectID": "index.html#documents",
    "href": "index.html#documents",
    "title": "Connectivity Analyses",
    "section": "Documents",
    "text": "Documents\n\nSample selectionDescriptives\n\n\nWe distributed the EURO Stoxx 600 firms to student assistants to extract information on the structure of annual reports. So far, this resulted in the following documents sample:\n\nno metadata in the SRN database yet,\nafter removing not readable pdfs,\npdfs where no text was scanned, and\npdfs with too little words(documents where the average clean text characters are less than 1,500 in a document).\n\n\n\n\n\n\n\n\n\nTable 1: Sample selection documents\n\n\n\n\nless\nresulting\n\n\n\n\n0\ndownloaded pdfs\n\n961\n\n\n1\nno metadata\n-53\n908\n\n\n2\nnot readable\n-168\n740\n\n\n3\nzero text scanned\n-28\n712\n\n\n4\ntoo little words*\n-102\n610\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Document descriptives"
  },
  {
    "objectID": "index.html#keywords-over-time",
    "href": "index.html#keywords-over-time",
    "title": "Connectivity Analyses",
    "section": "Keywords over time",
    "text": "Keywords over time\n\nF/S + Notes, Auditor’s ReportAll sectionsAll sections, Average per Page\n\n\n\n\n\n\n\n\nFigure 2: Development of average hits\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Development of average hits\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Development of hits per page"
  },
  {
    "objectID": "index.html#keyword-ranking",
    "href": "index.html#keyword-ranking",
    "title": "Connectivity Analyses",
    "section": "Keyword ranking",
    "text": "Keyword ranking\n\nRankingIn F/S + Notes and Auditor’s ReportBy all sections\n\n\n\n\n\n\n\n\nFigure 5: keyword ranking\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Keyword ranking by section\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Keyword ranking by section"
  },
  {
    "objectID": "index.html#relative-location-of-keywords-in-fs-notes",
    "href": "index.html#relative-location-of-keywords-in-fs-notes",
    "title": "Connectivity Analyses",
    "section": "Relative location of keywords in F/S + Notes",
    "text": "Relative location of keywords in F/S + Notes\nThis graph shows the relative location (0%: first page, 100%: last page) in the F/S + Notes section.\n\n\n\n\n\n\nFigure 8: Relative location"
  }
]